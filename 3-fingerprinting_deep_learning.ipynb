{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Identification Based on MEG Resting State Connectivity Matrices\n",
    "## License  \n",
    "This file is part of the project megFingerprinting. All of megFingerprinting code is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. megFingerprinting is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with megFingerprinting. If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get them libraries going\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature matrix from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle the dataset into one big matrix. For now I will use the first 50 subjects\n",
    "\n",
    "def prune_subject_csv(filename):\n",
    "    '''\n",
    "    This function takes in the subject's csv file from MATLAB, takes out auto-correlations, \n",
    "    doubled correlations (because of symmetry) and outputs a numpy array ready to be concatenated\n",
    "    in the grand feature matrix\n",
    "    Args:\n",
    "        filename (string): Name of the csv matrix\n",
    "    Returns: \n",
    "        sub_feat (np.array): Subject's features \n",
    "    '''\n",
    "    freq_band = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'hgamma']\n",
    "    sub_feat = np.zeros([1, int(((67 * 68)/2)*6+1)]) # Number of unique values in corr matrix + subject label\n",
    "    x = 0\n",
    "    for iFreq in freq_bands:\n",
    "        aec_matrix = pd.read_csv(filename, names = ['source', 'target', 'corr', 'freq_band'])\n",
    "        aec_matrix = aec_matrix.loc[aec_matrix['freq_band'] == iFreq]\n",
    "        aec_matrix = aec_matrix.replace('\\s', '', regex = True).pivot_table(index = 'source', columns = 'target', values = 'corr') \n",
    "        df_out = aec_matrix.stack()\n",
    "        df_out = df_out[df_out.index.get_level_values(0) != df_out.index.get_level_values(1)]\n",
    "        df_out = df_out[df_out.index.get_level_values(0) < df_out.index.get_level_values(1)]\n",
    "        df_out.index = df_out.index.map('_'.join)\n",
    "        sub_feat[0, x*2278 : ((x+1)*2278)] = df_out.to_frame().T.values\n",
    "        x += 1\n",
    "    sub_feat[0, -1] = int(filename[25:28])    \n",
    "    return sub_feat\n",
    "\n",
    "# Get 20 subjects: both training and testing datasets\n",
    "onlyfiles = [f for f in listdir('output/csv_matrices/') if isfile(join('output/csv_matrices/', f))]\n",
    "n_subs = 50 # Change here to get number of participants! \\\n",
    "sub_train = np.zeros([n_subs, int(((67 * 68)/2)*6+1)])\n",
    "sub_valid = np.zeros([n_subs, int(((67 * 68)/2)*6+1)])\n",
    "iv = 0\n",
    "it = 0\n",
    "for iFile in sorted(onlyfiles)[0:(n_subs*2)]: \n",
    "    sub = 'output/csv_matrices/' + iFile\n",
    "    if sub[39] == 'v':\n",
    "        sub_valid[iv, :] = prune_subject_csv(sub)\n",
    "        iv += 1\n",
    "    else:\n",
    "        sub_train[it, :] = prune_subject_csv(sub)\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate matrix into dependent and independent variables and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle arrays; no feature scaling because correlations\n",
    "X_train = sub_train[:, :-1]\n",
    "X_test = sub_valid[:, :-1]\n",
    "y_train = np.expand_dims(sub_train[:, -1], axis = 1)\n",
    "y_test = np.expand_dims(sub_valid[:, -1], axis = 1)\n",
    "\n",
    "# Encoding the dependent variable (validation and training)\n",
    "labelencoder_y_train = LabelEncoder()\n",
    "y_train[:, 0] = labelencoder_y_train.fit_transform(y_train[:, 0])\n",
    "onehotencoder = OneHotEncoder()\n",
    "y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "\n",
    "labelencoder_y_valid = LabelEncoder()\n",
    "y_test[:, 0] = labelencoder_y_valid.fit_transform(y_test[:, 0])\n",
    "onehotencoder = OneHotEncoder()\n",
    "y_test = onehotencoder.fit_transform(y_test).toarray()\n",
    "\n",
    "# Now, to avoid the dummy variable trap, we take away one of the three columns created by the hot encoder\n",
    "y_train = y_train[:, 1:]\n",
    "y_test = y_test[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 10s - loss: 14.5057 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      " - 7s - loss: 15.6024 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 4/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 5/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 6/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 7/50\n",
      " - 7s - loss: 15.2736 - acc: 0.0200\n",
      "Epoch 8/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 9/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 10/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 11/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 12/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 13/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 14/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 15/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 16/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 17/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 18/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 19/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 20/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 21/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 22/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 23/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 24/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 25/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 26/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 27/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 28/50\n",
      " - 7s - loss: 15.1515 - acc: 0.0400\n",
      "Epoch 29/50\n",
      " - 7s - loss: 15.4790 - acc: 0.0200\n",
      "Epoch 30/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 31/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 32/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 33/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 34/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 35/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 36/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 37/50\n",
      " - 7s - loss: 15.5924 - acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 39/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 40/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 41/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 42/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 43/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 44/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 45/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 46/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 47/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 48/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 49/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n",
      "Epoch 50/50\n",
      " - 7s - loss: 15.4734 - acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc4c7919b0>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "n_neurons = (X_train.shape[0] + X_train.shape[1]) / 2 # As a rule of thumb: hidden layer can have (input + ouput)/2 nodes\n",
    "\n",
    "# Adding the input layer hidden layers\n",
    "classifier.add(Dense(int(n_neurons), kernel_initializer = 'uniform', activation = 'relu', input_dim = 13668))\n",
    "classifier.add(Dropout(rate = 0.5))\n",
    "classifier.add(Dense(int(n_neurons/2), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(rate = 0.5))\n",
    "classifier.add(Dense(int(n_neurons/8), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(rate = 0.5))\n",
    "classifier.add(Dense(int(n_neurons/16), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(rate = 0.5))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(int(n_subs - 1), kernel_initializer = 'uniform', activation = 'softmax')) \n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "\n",
    "# Fit the ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size = int(n_subs/10), epochs = 50, verbose = 2) # Verbose 2 avoids the sliding bar, verbose = 0 no output at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "accuracy_score(y_test.argmax(axis = 1), y_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 12/08/18, the model keeps on overfitting (accuracy is at chance level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
